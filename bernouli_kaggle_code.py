# -*- coding: utf-8 -*-
"""bernouli (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EYx7OnWQKNGzeK79qwpGHuII8g1fpFje
"""

# %% [code]
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
import pickle

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# %% [code]
import random

# %% [code]
import re

# %% [code]
from collections import Counter

# %% [code]
from sklearn.model_selection import train_test_split

# %% [code]
#reading the stop words file
gist_file = open("/kaggle/input/stopwords/gist_stopwords.txt", "r")
try:
    content = gist_file.read()
    stopwords = content.split(",")
finally:
    gist_file.close()

# %% [code]
df=pd.read_csv('/kaggle/input/ift3395-6390-arxiv/train.csv')

# %% [code]
df_test=pd.read_csv('/kaggle/input/ift3395-6390-arxiv/test.csv')

# %% [code]
#preproccessing the text
def prep(X):
    documents=[]
    for sen in range(0, len(X)):
        # Remove all the special characters
        document = re.sub(r'\W', ' ', str(X[sen]))

        # remove all single characters
        document = re.sub(r'\s+[a-zA-Z]\s+', ' ', document)

        # Remove single characters from the start
        document = re.sub(r'\^[a-zA-Z]\s+', ' ', document) 

        # Substituting multiple spaces with single space
        document = re.sub(r'\s+', ' ', document, flags=re.I)

        # Removing prefixed 'b'
        document = re.sub(r'^b\s+', '', document)

        # Converting to Lowercase
        document = document.lower()

        document = document.split()
        
        #removing stop words and digits
        document = [word for word in document if (word not in stopwords) and word.isdigit()==False]
        document = ' '.join(document)

        documents.append(document)
    return documents

# %% [code]
df['prep_docs']=prep(df['Abstract'])

# %% [code]
#Build the frequency table
k=Counter(" ".join(docs).split())

# %% [code]
#construct the vocab greater than 3 to reduce the feature size
vocab = list(np.array(list(k.keys()))[np.array(list(k.values()))>3])

# %% [code]
#helper function for creating the count of word in a sentence
f = lambda x: Counter([y for y in x.split() if y in vocab])

# %% [code]
#building count feature set with the vocab 
#%%time

X = pd.DataFrame(df['prep_docs'].apply(f).values.tolist()).reindex(columns=vocab).fillna(0).astype(int).values
    

# %% [code]
df_test['prep_docs']=prep(df_test['Abstract'])

# %% [code]
#Building the test feature set 
#%%time
test_features = pd.DataFrame(df_test['prep_docs'].apply(f).values.tolist()).reindex(columns=vocab).fillna(0).astype(int).values

# %% [code]
#make the count of word > 1 to 1 for the bernouli NB
X[X>1]=1
test_features[test_features>1]=1

# %% [code]
#building the class map 
class_map=dict(zip(range(df['Category'].unique().shape[0]),df['Category'].unique()))
inverse_classmap=dict(zip(class_map.values(),class_map.keys()))

# %% [code]
#Converting categorical labels to numerical  
df['num_cat']=df['Category'].map(lambda x : inverse_classmap[x])

# %% [code]

y=df['num_cat'].tolist()

# %% [code]
#function to do the stratified shuffle split of data
def stratified_split(df):
    train=[]
    val=[]
    test=[]
    for i in df['Category'].unique():
        temp_df=df[df['Category']==i]
        fractions = np.array([0.80, 0.10, 0.10])
        # shuffle your input
        df = df.sample(frac=1) 
        # split into 3 parts
        train_c, val_c, test_c = np.array_split(
            temp_df, (fractions[:-1].cumsum() * len(temp_df)).astype(int))
        train+=list(train_c.index)
        val+=list(val_c.index)
        test+=list(test_c.index)
    return train, val,test

# %% [code]
#train,val,test=stratified_split(df)  #run it once for getting the indices and use the loading method below to get the indices from .pkl file

# %% [code]
#code to write the train,val,test indices to .pkl file for future usage
'''
with open('train_val_test_index.pkl', 'wb') as fp:
    pickle.dump((train,val,test), fp)'''

# %% [code]
#to avoid performing split again and to retain the train , val , test indexes 
with open ('train_val_test_index.pkl', 'rb') as fp:
    train,val,test = pickle.load(fp)

# %% [code]
print('The intersection of test and val is : ',set(val).intersection(set(test)))

# %% [code]
# splitting in to the train , val and test sets with 80, 10 ,10 ratio 
x_train,x_val,x_test=X[train],X[val],X[test]
y_train,y_val,y_test=df['num_cat'].loc[train].tolist(),df['num_cat'].loc[val].tolist(),df['num_cat'].loc[test].tolist()

# %% [code]
# class to implement Bernouli NB on the data.
#reused it from the special office hours 
class BernoulliNB:

    def __init__(self, alpha=1):
        self.alpha = alpha

    def fit(self, X, y):
        self.n_classes = len(np.unique(y))
        n_classes = self.n_classes

        # calculate P(C_k) for all k
        self.counts = np.zeros(n_classes)
        for i in y:
            self.counts[i] += 1
        self.counts /= len(y)


        # generate n_features x n_classes matrix
        self.params = np.zeros((n_classes, X.shape[1]))
        for idx in range(len(X)):
            self.params[y[idx]] += X[idx]
        self.params += self.alpha 

        # This is the correct thing to do
        self.class_sums = np.zeros(self.n_classes)
        for i in y:
            self.class_sums[i] += 1
        self.class_sums += self.n_classes*self.alpha # Laplace

        self.params = self.params / self.class_sums[:, np.newaxis]


    def predict(self, X):
        neg_prob = np.log(1 - self.params)
        # Compute  neg_prob · (1 - X).T  as  ∑neg_prob - X · neg_prob
        jll = np.dot(X, (np.log(self.params) - neg_prob).T)
        jll += np.log(self.counts) + neg_prob.sum(axis=1)
        return np.argmax(jll, axis=1)


# %% [code]
# select best value for alpha
for k in [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:
    model=BernoulliNB(alpha=k)
    model.fit(x_train,y_train)
    ypreds=model.predict(x_val)
    print('accuracy for validation on  alpha = '+str(k)+' is ', np.sum(ypreds==y_val)/len(y_val))
    

# %% [code]
#train the model on train_data + validation_data on alpha =0.2
model=BernoulliNB(alpha=0.2)
model.fit(np.concatenate((x_train,x_val)),y_train+y_val)


# %% [code]
#predicting for the test data
test_preds=model.predict(test_features)

# %% [code]
#converting Numerical labels to Categorical labels 
df_test['Category']=[class_map[i] for i in test_preds]

# %% [code]
#Generating csv 
df_test[['Id','Category']].to_csv('/kaggle/working/sub8_bernouli.csv',index=False)